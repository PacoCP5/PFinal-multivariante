---
title: " Estadística Multivariante. Práctica final."
author: Xu, Francisco y Gabriel
date: "`r Sys.Date()`"
output:
  html_document: 
    
    toc: true
    toc_depth: 5
    number_sections: false
    toc_float: 
      collapsed: false
      smooth_scroll: false
  pdf_document: default
---
<style>
.math {
  font-size: 8.25pt;options(encoding = 'UTF-8')
}
</style>

<div style="text-align: justify">

<!-- Ponemos la ruta de trabajo al directorio actual de RMarkdown -->
```{r echo=FALSE, include=FALSE, warning=FALSE}
#install.packages("rstudioapi")
library(rstudioapi)
#setwd(dirname(getActiveDocumentContext()$path))
```

<!-- Instalación y carga de librerías que usaremos -->
```{r echo=FALSE, include=FALSE, warning=FALSE}
#install.packages('cluster')
#install.packages('factoextra')
#install.packages('psych')
#install.packages('biotools')
library(biotools)
library(psych)
library(cluster)
library(factoextra)
library(reshape2)
library(knitr)
library(dplyr)
library(foreign)
library(ggcorrplot)
library(polycor)
library(corrplot)
library(ggplot2)
library(MVN)
library(MASS)
```

Cargamos nuestro csv:
```{r echo=TRUE, include=TRUE, warning=FALSE}
datos <- read.csv("diabetes.csv")
original_datos <- datos
```

## Análisis exploratorio univariante

Con la función summary, podemos ver las medidas de dispersión:
```{r echo=TRUE, include=TRUE, warning=FALSE}
summary(datos)
```

Veamos los outliers existentes apoyándonos en boxplot:
```{r echo=TRUE, include=TRUE, warning=FALSE}
par(mfrow=c(1,1))
boxplot(original_datos, main="Análisis exploratorio de datos")
```

Sustituimos estos outliers por la media con la siguiente función:
```{r echo=TRUE, include=TRUE, warning=FALSE}
outlier2<-function(data,na.rm=T) {
  H<-1.5*IQR(data)
  
  data[data<quantile(data,0.25,na.rm = T)-H]<-NA
  
  data[data>quantile(data,0.75, na.rm = T)+H]<-NA
  
  data[is.na(data)]<-mean(data, na.rm = T)
  
  H<-1.5*IQR(data)
  
  if (TRUE %in% (data<quantile(data,0.25,na.rm = T)-H) | TRUE %in% (data>quantile(data,0.75,na.rm = T)+H))
    outlier2(data)
  else
    return(data)
}
```

```{r echo=TRUE, include=TRUE, warning=FALSE}
  for(i in 1:ncol(datos)){
    datos[,i] <- outlier2(datos[,i]) 
  }
```

```{r echo=TRUE, include=TRUE, warning=FALSE}
  par(mfrow=c(1,2))
  boxplot(original_datos, main="Datos originales")
  boxplot(datos, main="Datos sin outliers")
```


**Distribuciones individuales**
```{r echo=TRUE, include=TRUE, warning=FALSE}
# Representación mediante Histograma 
par(mfcol = c(3, 3))
for (k in 1:ncol(datos)) {
  j0 <- names(datos)[k]
    x0 <- seq(min(datos[, k]), max(datos[, k]), le = 50)
    x <- datos[,k]
    hist(x, proba = T, col = grey(0.8), main = j0, xlab = j0)
    lines(x0, dnorm(x0, mean(x), sd(x)), col = "red", lwd = 2)
}
```


**Gráficos qqplots**
```{r echo=TRUE, include=TRUE, warning=FALSE}
# Representación de cuantiles normales de cada variable 
par(mfrow=c(3,3))
for (k in 1:ncol(datos)) {
  j0 <- names(datos)[k]
  x <- datos[,k]
  qqnorm(x, main = j0, pch = 19, col=k+1)
  qqline(x)
}
```

Si bien este análisis exploratorio puede darnos una idea de la posible normalidad o no de las variables univariadas, siempre es mejor hacer los respectivos test de normalidad.

**Test de normalidad univariantes (Shapiro-Wilk)**
```{r echo=TRUE, include=TRUE, warning=FALSE}
datos_tidy <- melt(datos, value.name = "valor", id.vars= NULL)
aggregate(valor~variable, data=datos_tidy, FUN = function(x){shapiro.test(x)$p.value})
```


## Análisis exploratorio multivariante

**Análisis de la correlación**

Si las variables no guardan ninguna correlación, no tendría sentido aplicar técnicas como ACP o AF, 
po lo que el primer paso es comprobar esto.

Primero, veamos la presencia de correlación gráficamente:
```{r echo=TRUE, include=TRUE, warning=FALSE}
poly_cor<-hetcor(datos)$correlations
ggcorrplot(poly_cor, type="lower",hc.order=T)
```

```{r echo=TRUE, include=TRUE, warning=FALSE}
par(mfrow=c(1,1))
corrplot(cor(datos), order = "hclust", tl.col='black', tl.cex=1)
```

Acto seguido, procederemos con el test de Bartlett, el cual parte de la hipótesis nula de que la matriz de covarianzas
es la identidad, esto es, las variables son independientes.
```{r echo=TRUE, include=TRUE, warning=FALSE}
# Se normalizan los datos
datos_normalizados<-scale(datos)
# Se hace el test de esfericidad
cortest.bartlett(cor(datos_normalizados), n = nrow(datos))
```
Como podemos observar, el test sale significativo y por ende la hipótesis nula que afirma que la matriz de correlaciones es la identidad queda descartada.
Al haber relación entre las variables, tiene sentido plantearse análisis ACP o AF.

# Reducción de la dimensión mediante un análisis de componentes principales

Procedemos pues a aplicarle ACP ya que previamente hemos arreglado los datos outliers.
```{r echo=TRUE, include=TRUE, warning=FALSE}
# Realización del ACP
PCA<-prcomp(datos, scale=T, center = T)

# El campo "rotation" del objeto "PCA" es una matriz cuyas columnas
# son los coeficientes de las componentes principales, es decir, el
# peso de cada variable en la correspondiente componente principal
PCA$rotation

# En el campo "sdev" del objeto "PCA" y con la función summary aplicada
# al objeto, obtenemos información relevante: desviaciones típicas de 
# cada componente principal, proporción de varianza explicada y acumulada.
PCA$sdev
summary(PCA)
```  

```{r echo=TRUE, include=TRUE, warning=FALSE}
par(mfrow=c(1,1))

# Proporción de varianza explicada
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:ncol(datos)),
       aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Componente principal", y= " Proporción de varianza explicada")
```

```{r echo=TRUE, include=TRUE, warning=FALSE}
# Proporción de varianza explicada acumulada
varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:ncol(datos)),
        aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
  geom_col(width = 0.5) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Proporción varianza explicada acumulada")
```  

Ahora debemos elegir cuántas componentes principales vamos a elegir. Utilizaremos la Regla de Abdi para ver la media de las varianzas explicadas por las componentes principales y seleccionar aquellas cuya proporción de varianza explicada supere a dicha media.

```{r echo=TRUE, include=TRUE, warning=FALSE}
PCA$sdev^2
mean(PCA$sdev^2)
```

Podemos observar que solo las 3 primeras componentes principales superan la media obtenida, aunque consideramos que incluir la cuarta componente sería beneficioso dada la cercanía a la media y que con solo 3 componentes principales la varianza acumulada explicada sería de 0.5788 mientras que añadiendo la cuarta sería de 0.6898.

A continuación, apoyaremos esta decisión aplicando el método gráfico del codo.
Graficaremos las varianzas acumaladas y observaremos cuándo su crecimiento se estanca.
```{r echo=TRUE, include=TRUE, warning=FALSE}
fviz_nbclust(scale(datos), kmeans, method = 'wss')
```

# Reducción de la dimensión mediante un análisis factorial
Como ya hemos visto en el apartado anterior, tiene sentido plantear un análisis factorial.
```{r echo=TRUE, include=TRUE, warning=FALSE}
scree(poly_cor)
fa.parallel(poly_cor,n.obs=200,fa="fa",fm="minres") 
```

Observando el gráfico y el resultado del análisis paralelo, el número óptimo de factores es 4.

```{r echo=TRUE, include=TRUE, warning=FALSE}
modelo1<-fa(poly_cor,
            nfactors = 4,
            rotate = "none",
            fm="mle") # modelo m?xima verosimilitud

modelo2<-fa(poly_cor,
            nfactors = 4,
            rotate = "none",
            fm="minres") # modelo m?nimo residuo
```

Comparando las comunalidades
```{r echo=TRUE, include=TRUE, warning=FALSE}
sort(modelo1$communality,decreasing = T)->c1
sort(modelo2$communality,decreasing = T)->c2
head(cbind(c1,c2))
```

Comparacion de las unicidades, es decir la proporción de varianza que no ha sido explicada por el factor (1-comunalidad)
```{r echo=TRUE, include=TRUE, warning=FALSE}
sort(modelo1$uniquenesses,decreasing = T)->u1
sort(modelo2$uniquenesses,decreasing = T)->u2
head(cbind(u1,u2))
```

# Análisis discriminante: Clasificadores

## Análisis de la normalidad multivariante

```{r echo=TRUE, include=TRUE, warning=FALSE}
#outliers <- mvn(data = datos[,-1], mvnTest = "hz", multivariateOutlierMethod = "quan")
```

```{r echo=TRUE, include=TRUE, warning=FALSE}
royston_test <- mvn(data = datos[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality

hz_test <- mvn(data = datos[,-1], mvnTest = "hz")
hz_test$multivariateNormality

```


## Clasificadores

### Lineal
```{r echo=TRUE, include=TRUE, warning=FALSE}
modelo_lda <- lda(formula = Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age,data = datos)
modelo_lda
```

### Cuadrático
```{r echo=TRUE, include=TRUE, warning=FALSE}
modelo_qda <- qda(formula = Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age,data = datos)
modelo_qda
```

## Resultados de los Clasificadores

```{r echo=TRUE, include=TRUE, warning=FALSE}
pred <- predict(modelo_lda, dimen = 1)
confusionmatrix(datos$Outcome, pred$class)

# Porcentaje de errores de clasificación
trainig_error <- mean(datos$Outcome != pred$class) * 100
paste("trainig_error=", trainig_error, "%")
```

```{r echo=TRUE, include=TRUE, warning=FALSE}
pred <- predict(modelo_qda, dimen = 1)
confusionmatrix(datos$Outcome, pred$class)

# Porcentaje de errores de clasificación
trainig_error <- mean(datos$Outcome != pred$class) * 100
paste("trainig_error=", trainig_error, "%")
```